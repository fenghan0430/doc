## 2023/11/15

神经网络是特征交叉的复杂版本。

## 2023/11/13

精确率，召回率和F值

**精确率**是在所有模型判定为正类的样本中，实际上真正为正类的样本所占的比例。 

**召回率**是在所有实际为正类的样本中，被模型正确识别出来的比例。

**F值**在精确率和召回率之间提供了一个平衡，是这两者的综合性能度量。F值越高，模型的性能通常被认为越好。

**交叉验证**是把数据分成两份，一些用于测试评估模型，另外大部分用来训练模型。具体怎么区分看比例。

## 2023/11/8

当您使用类别不平衡的数据集（例如正类别标签和负类别标签的数量之间存在明显差异）时，单单准确率一项并不能反映全面情况。

逻辑回归会返回一个概率，即大于0.5为真，小于0.5为假。但有时候，分类并不是以0.5为界，也可能是0.114等，根据具体情况，设定**分类阈值**（也称为**决策阈值**）。

*目标函数*之**似然函数**，我们知道逻辑回归是以概率为判断依据，我举个例子来理解：想一想扔 2 次骰子的情况。第 1 次的结果是 1 点，且第 2 次的结果是 2 点的概率是多少呢？首先 1 点出现的概率是六分之一，接下来 2 点出现的概率是六分之一，二者连续发生的概率就要使用乘法计算：六分之一 * 六分之一 = 三十六分之一。这就是扔两次骰子，第一次是1点，第二次是2点的情况的概率。逻辑回归的概率也是分数（或者是小数），把训练数据的预期概率相乘，就得到

**似然函数**，是一种统计模型中关于参数的函数。

## 2023/11/7

**目标函数**和损失函数差不多(均方误差就是损失函数)。

**最小二乘法**不是指一种具体的做法，而是指一种思想，最小二乘法就是使得误差平方和最小。用什么方法都可以！其中均方误差是比较常见使用的损失函数，所以在最小二乘法的解释中最为常用。

[最小二乘法解析](https://blog.csdn.net/MoreAction_/article/details/106443383)

## 2023/10/20
**逻辑回归**把分类作为概率来考虑(如果一个检测形状的ai，认为一个物体是方形的概率是80%，是圆柱形的概率是20%。那么最终答案是方形)。
**决策边界**是用于数据分类的直线(把线性回归想象成一种xy坐标图，那么训练模型就是为了找到一条直线(这条直线就是和权重向量垂直的直线)，这条直线的左边是A，右边是B，那么这条线就叫决策边界)

## 2023/10/19
**线性可分**指可以用一条直线分类的问题。

## 2023/9/22
**导数**描述了一个函数在某一点处的瞬时变化率，或者说是函数在这一点的切线斜率。

## 2023/9/20
**批量梯度下降**为了解决单个梯度下降的效率低，从而提出的方法。批量梯度下降

## 2023/9/14
**感知机**是接受多个输入后将每个值与 各自的权重相乘，最后输出总和的模型，它的缺点就是它只能解决线性可分的问题。
机器学习中的权重是相当与统线方程中的“斜率”概念相同

机器学习中的每一个数据是一个**样本**

模型损失变化的非常缓慢表示**模型已收敛**